{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch13. 텐서플로에서 데이터 적재와 전처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대규모 데이터셋을 효율적으로 로드하고 전처리 하기\n",
    "- dataset 객체를 만들고 데이터 읽어올 위치와 변환 방법 지정\n",
    "- 멀티스레딩, 큐, 배치, 프리패치 등 대신 처리\n",
    "- tf.keras와도 잘 동작\n",
    "\n",
    "**Chapter 13 – Loading and Preprocessing Data with TensorFlow**\n",
    "\n",
    "_This notebook contains all the sample code and solutions to the exercises in chapter 13._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/ageron/handson-ml2/blob/master/13_loading_and_preprocessing_data.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    !pip install -q -U tfx==0.21.2\n",
    "    print(\"You can safely ignore the package incompatibility errors.\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"data\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.1 데이터 API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset 개념: 연속된 데이터 샘플\n",
    "X = tf.range(10)\n",
    "\n",
    "# tf.data.Dataset.from_tensor_slices()\n",
    "# 입력: tensor\n",
    "# 출력: tf.data.Dataset -> (첫번째 차원을 따라) X의 각 원소가 item으로 표현됨\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# item 순회하기\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위와 동일한 dataset 만들기\n",
    "dataset = tf.data.Dataset.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.1.1 연쇄변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여러 종류 변환 메서드 수행 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# [dataset].repeat(n): dataset의 item을 n번 반복하는 새로운 dataset을 반환\n",
    "# [dataset].batch(k): dataset의 item을 k개씩 그룹으로 묶음, 마지막 배치크기는 남은것으로 구성되 다를수 있음\n",
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_remainder=True // 길이가 모자란 마지막 배치를 버림. 모든 배치 동일 크기\n",
    "#dataset = dataset.repeat(3).batch(7, drop_remainder=True)\n",
    "#for item in dataset:\n",
    "#    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 item에 2 곱하여 새로운 dataset 만들기\n",
    "# map() \n",
    "# dataset의 item 변환하기\n",
    "# num_parallel_cells 매개변수: 여러 스레드로 나누어 속도를 높이는 전처리 작업 가능\n",
    "# map 전달 함수: 텐서플로 함수로 변환 가능해야함\n",
    "dataset = dataset.map(lambda x: x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int64)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int64)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#dataset = dataset.apply(tf.data.experimental.unbatch()) # Now deprecated\\ndataset = dataset.unbatch()\\nfor item in dataset:\\n    print(item)\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply() \n",
    "# dataset 전체에 변환 적용\n",
    "# unbatch() 적용하기 -> \n",
    "\"\"\"\n",
    "#dataset = dataset.apply(tf.data.experimental.unbatch()) # Now deprecated\n",
    "dataset = dataset.unbatch()\n",
    "for item in dataset:\n",
    "    print(item)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndataset = dataset.filter(lambda x: x < 10)  # keep only items < 10\\nfor item in dataset:\\n    print(item)\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter()\n",
    "# dataset 필터링 하기\n",
    "\"\"\"\n",
    "dataset = dataset.filter(lambda x: x < 10)  # keep only items < 10\n",
    "for item in dataset:\n",
    "    print(item)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int64)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# take()\n",
    "# dataset의 몇개 item만 보기\n",
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.1.2 데이터 셔플링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 경사하강법은 train set에 있는 sample이 독립적이고, 동일한 분포일 때 최고의 성능 발휘 \n",
    "- 방법: shuffle() 이용해 sample 섞기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "dataset = tf.data.Dataset.range(10).repeat(3) #0~9까지 3번 반복\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 6 5 7 3 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 2 1 0 4 6 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 2 5 9 2 1 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 3 8 7 9 5 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 6], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# shuffle()\n",
    "# 원본 dataset의 처음 item을 buffer_size 만큼 추출하여 buffer에 채움\n",
    "# 새로운 item이 요청되면, 이 buffer에서 랜덤하게 하나 꺼내 반환\n",
    "# -> 원본 dataset에서 비워진 buffer 채움\n",
    "# -> 원본 dataset의 모든 item이 사용될 떄까지 반복\n",
    "# -> 그다음엔 buffer가 비워질 때까지 계속 랜덤하게 item 반환\n",
    "# buffer_size: 충분히 크게 해야 셔플링 효과(메모리 크기 넘지 않아야 하고, dataset보다 클 필요 없음)\n",
    "# seed: 실행할 때마다 셔플링 되는 순서를 동일하게 만들기 위해 랜덤시드 부여\n",
    "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 메모리 용량보다 데이터가 큰 경우\n",
    "    - 원본 데이터 자체를 섞음 (ex. 리눅스 shuf 명령어로 데이터 섞기)\n",
    "        - epoch 마다도 한번 더 섞음 -> epoch 마다 동일 순서 반복되 모델 편향 생기지 않게\n",
    "\n",
    "    - 또는 원본 데이터를 여러 파일로 나눈 다음 훈련하는 동안 무작위로 읽음\n",
    "        - 동일 파일의 샘플이 함께 처리되는 것 막기 위해, \n",
    "            - 파일 여러개를 무작위 선택하고 동시에 읽은 record를 돌아가면서 반환\n",
    "            - 그 다음, shuffle() 메서드로 셔플링 버퍼 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the California dataset to multiple CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading and preparing the California housing dataset. We first load it, then split it into a training set, a validation set and a test set, and finally we scale it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 8)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.data.shape #(20640, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640,)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.target.shape #(20640,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 1)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reshape(-1, 1)\n",
    "#(20640,)->(20640, 1)\n",
    "#1차원(k,) -> 2차원(k,1)     \n",
    "reshape = housing.target.reshape(-1, 1)\n",
    "reshape.shape #(20640, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test 분할\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "##train에서 train/valid 분할\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (11610, 8)\n",
      "X_valid: (3870, 8)\n",
      "X_test: (5160, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_valid:\", X_valid.shape)\n",
    "print(\"X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler X_mean:\n",
      " [ 3.89175860e+00  2.86245478e+01  5.45593655e+00  1.09963474e+00\n",
      "  1.42428122e+03  2.95886657e+00  3.56464315e+01 -1.19584363e+02]\n",
      "StandardScaler X_std:\n",
      " [1.90927329e+00 1.26409177e+01 2.55038070e+00 4.65460128e-01\n",
      " 1.09576000e+03 2.36138048e+00 2.13456672e+00 2.00093304e+00]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_\n",
    "print(\"StandardScaler X_mean:\\n\", X_mean)\n",
    "print(\"StandardScaler X_std:\\n\", X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a very large dataset that does not fit in memory, you will typically want to split it into many files first, then have TensorFlow read these files in parallel. To demonstrate this, let's start by splitting the housing dataset and save it to 20 CSV files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    #make folder\n",
    "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    \n",
    "    #make csv file\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        \n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "target_name = \"MedianHouseValue\"\n",
    "header_cols = housing.feature_names + [target_name]\n",
    "header = \",\".join(header_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/valid/test set의 각 set을 여러개의 csv 파일로 나눔\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets/housing/my_train_00.csv',\n",
       " 'datasets/housing/my_train_01.csv',\n",
       " 'datasets/housing/my_train_02.csv',\n",
       " 'datasets/housing/my_train_03.csv',\n",
       " 'datasets/housing/my_train_04.csv',\n",
       " 'datasets/housing/my_train_05.csv',\n",
       " 'datasets/housing/my_train_06.csv',\n",
       " 'datasets/housing/my_train_07.csv',\n",
       " 'datasets/housing/my_train_08.csv',\n",
       " 'datasets/housing/my_train_09.csv',\n",
       " 'datasets/housing/my_train_10.csv',\n",
       " 'datasets/housing/my_train_11.csv',\n",
       " 'datasets/housing/my_train_12.csv',\n",
       " 'datasets/housing/my_train_13.csv',\n",
       " 'datasets/housing/my_train_14.csv',\n",
       " 'datasets/housing/my_train_15.csv',\n",
       " 'datasets/housing/my_train_16.csv',\n",
       " 'datasets/housing/my_train_17.csv',\n",
       " 'datasets/housing/my_train_18.csv',\n",
       " 'datasets/housing/my_train_19.csv']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3872\r\n",
      "drwxr-xr-x  42 csg  staff   1344  8 23 14:36 \u001b[1m\u001b[36m.\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   3 csg  staff     96  8 23 14:36 \u001b[1m\u001b[36m..\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 csg  staff  47327  8 23 14:36 my_test_00.csv\r\n",
      "-rw-r--r--   1 csg  staff  47387  8 23 14:36 my_test_01.csv\r\n",
      "-rw-r--r--   1 csg  staff  47330  8 23 14:36 my_test_02.csv\r\n",
      "-rw-r--r--   1 csg  staff  47457  8 23 14:36 my_test_03.csv\r\n",
      "-rw-r--r--   1 csg  staff  47385  8 23 14:36 my_test_04.csv\r\n",
      "-rw-r--r--   1 csg  staff  47423  8 23 14:36 my_test_05.csv\r\n",
      "-rw-r--r--   1 csg  staff  47509  8 23 14:36 my_test_06.csv\r\n",
      "-rw-r--r--   1 csg  staff  47403  8 23 14:36 my_test_07.csv\r\n",
      "-rw-r--r--   1 csg  staff  47405  8 23 14:36 my_test_08.csv\r\n",
      "-rw-r--r--   1 csg  staff  47453  8 23 14:36 my_test_09.csv\r\n",
      "-rw-r--r--   1 csg  staff  53289  8 23 14:36 my_train_00.csv\r\n",
      "-rw-r--r--   1 csg  staff  53417  8 23 14:36 my_train_01.csv\r\n",
      "-rw-r--r--   1 csg  staff  53471  8 23 14:36 my_train_02.csv\r\n",
      "-rw-r--r--   1 csg  staff  53347  8 23 14:36 my_train_03.csv\r\n",
      "-rw-r--r--   1 csg  staff  53730  8 23 14:36 my_train_04.csv\r\n",
      "-rw-r--r--   1 csg  staff  53602  8 23 14:36 my_train_05.csv\r\n",
      "-rw-r--r--   1 csg  staff  53415  8 23 14:36 my_train_06.csv\r\n",
      "-rw-r--r--   1 csg  staff  53248  8 23 14:36 my_train_07.csv\r\n",
      "-rw-r--r--   1 csg  staff  53329  8 23 14:36 my_train_08.csv\r\n",
      "-rw-r--r--   1 csg  staff  53279  8 23 14:36 my_train_09.csv\r\n",
      "-rw-r--r--   1 csg  staff  53043  8 23 14:36 my_train_10.csv\r\n",
      "-rw-r--r--   1 csg  staff  53222  8 23 14:36 my_train_11.csv\r\n",
      "-rw-r--r--   1 csg  staff  53248  8 23 14:36 my_train_12.csv\r\n",
      "-rw-r--r--   1 csg  staff  53219  8 23 14:36 my_train_13.csv\r\n",
      "-rw-r--r--   1 csg  staff  53479  8 23 14:36 my_train_14.csv\r\n",
      "-rw-r--r--   1 csg  staff  53503  8 23 14:36 my_train_15.csv\r\n",
      "-rw-r--r--   1 csg  staff  53445  8 23 14:36 my_train_16.csv\r\n",
      "-rw-r--r--   1 csg  staff  53520  8 23 14:36 my_train_17.csv\r\n",
      "-rw-r--r--   1 csg  staff  53729  8 23 14:36 my_train_18.csv\r\n",
      "-rw-r--r--   1 csg  staff  53227  8 23 14:36 my_train_19.csv\r\n",
      "-rw-r--r--   1 csg  staff  35573  8 23 14:36 my_valid_00.csv\r\n",
      "-rw-r--r--   1 csg  staff  35654  8 23 14:36 my_valid_01.csv\r\n",
      "-rw-r--r--   1 csg  staff  35661  8 23 14:36 my_valid_02.csv\r\n",
      "-rw-r--r--   1 csg  staff  35837  8 23 14:36 my_valid_03.csv\r\n",
      "-rw-r--r--   1 csg  staff  35686  8 23 14:36 my_valid_04.csv\r\n",
      "-rw-r--r--   1 csg  staff  35712  8 23 14:36 my_valid_05.csv\r\n",
      "-rw-r--r--   1 csg  staff  35545  8 23 14:36 my_valid_06.csv\r\n",
      "-rw-r--r--   1 csg  staff  35850  8 23 14:36 my_valid_07.csv\r\n",
      "-rw-r--r--   1 csg  staff  35652  8 23 14:36 my_valid_08.csv\r\n",
      "-rw-r--r--   1 csg  staff  35697  8 23 14:36 my_valid_09.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./datasets/housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's take a peek at the first few lines of one of these CSV files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/housing/my_train_00.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedianHouseValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.5214</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.049945</td>\n",
       "      <td>1.106548</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>1.605993</td>\n",
       "      <td>37.63</td>\n",
       "      <td>-122.43</td>\n",
       "      <td>1.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.3275</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.490060</td>\n",
       "      <td>0.991054</td>\n",
       "      <td>3464.0</td>\n",
       "      <td>3.443340</td>\n",
       "      <td>33.69</td>\n",
       "      <td>-117.39</td>\n",
       "      <td>1.687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.1000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7.542373</td>\n",
       "      <td>1.591525</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>2.250847</td>\n",
       "      <td>38.44</td>\n",
       "      <td>-122.98</td>\n",
       "      <td>1.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.1736</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.289003</td>\n",
       "      <td>0.997442</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>2.695652</td>\n",
       "      <td>33.55</td>\n",
       "      <td>-117.70</td>\n",
       "      <td>2.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.0549</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.312457</td>\n",
       "      <td>1.085092</td>\n",
       "      <td>3297.0</td>\n",
       "      <td>2.244384</td>\n",
       "      <td>33.93</td>\n",
       "      <td>-116.93</td>\n",
       "      <td>0.956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  3.5214      15.0  3.049945   1.106548      1447.0  1.605993     37.63   \n",
       "1  5.3275       5.0  6.490060   0.991054      3464.0  3.443340     33.69   \n",
       "2  3.1000      29.0  7.542373   1.591525      1328.0  2.250847     38.44   \n",
       "3  7.1736      12.0  6.289003   0.997442      1054.0  2.695652     33.55   \n",
       "4  2.0549      13.0  5.312457   1.085092      3297.0  2.244384     33.93   \n",
       "\n",
       "   Longitude  MedianHouseValue  \n",
       "0    -122.43             1.442  \n",
       "1    -117.39             1.687  \n",
       "2    -122.98             1.621  \n",
       "3    -117.70             2.621  \n",
       "4    -116.93             0.956  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file = train_filepaths[0]\n",
    "print(file)\n",
    "pd.read_csv(file).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets/housing/my_train_00.csv',\n",
       " 'datasets/housing/my_train_01.csv',\n",
       " 'datasets/housing/my_train_02.csv',\n",
       " 'datasets/housing/my_train_03.csv',\n",
       " 'datasets/housing/my_train_04.csv',\n",
       " 'datasets/housing/my_train_05.csv',\n",
       " 'datasets/housing/my_train_06.csv',\n",
       " 'datasets/housing/my_train_07.csv',\n",
       " 'datasets/housing/my_train_08.csv',\n",
       " 'datasets/housing/my_train_09.csv',\n",
       " 'datasets/housing/my_train_10.csv',\n",
       " 'datasets/housing/my_train_11.csv',\n",
       " 'datasets/housing/my_train_12.csv',\n",
       " 'datasets/housing/my_train_13.csv',\n",
       " 'datasets/housing/my_train_14.csv',\n",
       " 'datasets/housing/my_train_15.csv',\n",
       " 'datasets/housing/my_train_16.csv',\n",
       " 'datasets/housing/my_train_17.csv',\n",
       " 'datasets/housing/my_train_18.csv',\n",
       " 'datasets/housing/my_train_19.csv']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or in text mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n",
      "3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n",
      "5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n",
      "3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n",
      "7.1736,12.0,6.289002557544757,0.9974424552429667,1054.0,2.6956521739130435,33.55,-117.7,2.621\n"
     ]
    }
   ],
   "source": [
    "with open(file) as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline(), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets/housing/my_train_00.csv',\n",
       " 'datasets/housing/my_train_01.csv',\n",
       " 'datasets/housing/my_train_02.csv',\n",
       " 'datasets/housing/my_train_03.csv',\n",
       " 'datasets/housing/my_train_04.csv',\n",
       " 'datasets/housing/my_train_05.csv',\n",
       " 'datasets/housing/my_train_06.csv',\n",
       " 'datasets/housing/my_train_07.csv',\n",
       " 'datasets/housing/my_train_08.csv',\n",
       " 'datasets/housing/my_train_09.csv',\n",
       " 'datasets/housing/my_train_10.csv',\n",
       " 'datasets/housing/my_train_11.csv',\n",
       " 'datasets/housing/my_train_12.csv',\n",
       " 'datasets/housing/my_train_13.csv',\n",
       " 'datasets/housing/my_train_14.csv',\n",
       " 'datasets/housing/my_train_15.csv',\n",
       " 'datasets/housing/my_train_16.csv',\n",
       " 'datasets/housing/my_train_17.csv',\n",
       " 'datasets/housing/my_train_18.csv',\n",
       " 'datasets/housing/my_train_19.csv']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Dataset.list_files()\n",
    "# 파일 경로를 섞은 dataset을 반환\n",
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'datasets/housing/my_train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_08.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_13.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for filepath in filepath_dataset:\n",
    "    print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suffle 원하지 않는 경우, shuffle=False\n",
    "filepath_dataset_no = tf.data.Dataset.list_files(train_filepaths, shuffle=False, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'datasets/housing/my_train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_08.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_19.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for filepath in filepath_dataset_no:\n",
    "    print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 번에 n개 파일을 한 줄씩 번갈아 읽음\n",
    "# 각 파일의 첫줄은 열 이름으로 skip() 이용\n",
    "n_readers = 5\n",
    "\n",
    "# interleave()\n",
    "# filepath_dataset에 있는 5개의 파일 경로에서 데이터를 읽는 데이터셋을 만듦\n",
    "# 각 파일에 람다 함수를 적용해 새로운 데이터 셋을 만듦(TextLineDataset -> 각 줄)\n",
    "# 5개 파일씩 전체 파일을 다 볼때까지 반복\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'4.6477,38.0,5.03728813559322,0.911864406779661,745.0,2.5254237288135593,32.64,-117.07,1.504'\n",
      "b'8.72,44.0,6.163179916317992,1.0460251046025104,668.0,2.794979079497908,34.2,-118.18,4.159'\n",
      "b'3.8456,35.0,5.461346633416459,0.9576059850374065,1154.0,2.8778054862842892,37.96,-122.05,1.598'\n",
      "b'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526'\n",
      "b'3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625'\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.1.3 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련 세트에 있는 각 특성과 표준 편차\n",
    "#특성마다 1개씩 8개 실수를 가진 1D 텐서\n",
    "#X_mean, X_std = [...] \n",
    "n_inputs = 8 # X_train.shape[-1]\n",
    "\n",
    "# preprocess()\n",
    "# CSV 한 라인을 받아 파싱함\n",
    "@tf.function\n",
    "def preprocess(line):\n",
    "    # 모든 특성 열이 실수, 누락된 값의 기본값 0으로 설정\n",
    "    # 마지막 열(target)은 tf.float32타입의 빈 배열을 제공 -> 해당 열은 실수이지만, 기본값이 없음. 누락값 발생 시 에러 발생\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    print(\"defs:\", defs)\n",
    "    \n",
    "    # decode_csv()\n",
    "    # 매개변수: 파싱할 라인, CSV파일의 각 열에 대한 기본값을 담은 배열(각 기본값, 열 개수, 데이터 타입도 알려줌)\n",
    "    # 반환값: (열마다 한 개씩) 스칼라 텐서의 리스트를 반환\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    print(\"fields:\", fields)\n",
    "    \n",
    "    # tf.stcck()\n",
    "    # 마지막 열()을 제외하고 모든 텐서를 쌓아 1D 배열을 만들어 반환\n",
    "    x = tf.stack(fields[:-1])\n",
    "    print(\"x:\", x)\n",
    "    # target에도 적용 \n",
    "    y = tf.stack(fields[-1:])\n",
    "    print(\"y:\", y)\n",
    "    \n",
    "    # 입력 특성에서 평균 빼고 표준 편차로 나누어 스케일을 조정\n",
    "    scaled_x = (x - X_mean) / X_std\n",
    "    \n",
    "    # 스케일 조정된 특성과 target 담긴 튜플 반환\n",
    "    return scaled_x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defs: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, <tf.Tensor 'Const:0' shape=(0,) dtype=float32>]\n",
      "fields: [<tf.Tensor 'DecodeCSV:0' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:1' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:2' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:3' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:4' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:5' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:6' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:7' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:8' shape=() dtype=float32>]\n",
      "x: Tensor(\"stack:0\", shape=(8,), dtype=float32)\n",
      "y: Tensor(\"stack_1:0\", shape=(1,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=3288, shape=(8,), dtype=float32, numpy=\n",
       " array([ 0.16579157,  1.216324  , -0.05204565, -0.39215982, -0.5277444 ,\n",
       "        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\n",
       " <tf.Tensor: id=3289, shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.1.4 데이터 적재와 전처리를 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 헬퍼 함수\n",
    "# CSV파일에서 데이터를 효율적으로 적재하고, 전처리, 셔플링, 반복, 배치를 적용한 데이터셋 만들어 반환\n",
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n",
    "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
    "                       n_parse_threads=5, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = tf.Tensor(\n",
      "[[ 0.5804519  -0.20762321  0.05616303 -0.15191229  0.01343246  0.00604472\n",
      "   1.2525111  -1.3671792 ]\n",
      " [ 5.818099    1.8491895   1.1784915   0.28173092 -1.2496178  -0.3571987\n",
      "   0.7231292  -1.0023477 ]\n",
      " [-0.9253566   0.5834586  -0.7807257  -0.28213993 -0.36530012  0.27389365\n",
      "  -0.76194876  0.72684526]], shape=(3, 8), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[1.752]\n",
      " [1.313]\n",
      " [1.535]], shape=(3, 1), dtype=float32)\n",
      "\n",
      "X = tf.Tensor(\n",
      "[[-0.8324941   0.6625668  -0.20741376 -0.18699841 -0.14536144  0.09635526\n",
      "   0.9807942  -0.67250353]\n",
      " [-0.62183803  0.5834586  -0.19862501 -0.3500319  -1.1437552  -0.3363751\n",
      "   1.107282   -0.8674123 ]\n",
      " [ 0.8683102   0.02970133  0.3427381  -0.29872298  0.7124906   0.28026953\n",
      "  -0.72915536  0.86178064]], shape=(3, 8), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[0.919]\n",
      " [1.028]\n",
      " [2.182]], shape=(3, 1), dtype=float32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "train_set = csv_reader_dataset(train_filepaths, batch_size=3)\n",
    "for X_batch, y_batch in train_set.take(2):\n",
    "    print(\"X =\", X_batch)\n",
    "    print(\"y =\", y_batch)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.1.5 Prefetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* prefetch(1)\n",
    "    - 데이터 셋은 항상 한 배치가 미리 준비되도록 준비\n",
    "    - 훈련 알고리즘이 한 배치로 작업을 하는 동안 데이터 셋이 동시에 다음 배치를 준비(디스크에서 데이터를 읽고 전처리)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 13.1.6 tf.keras와 데이터셋 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련, 검증, 테스트 세트 만들기\n",
    "train_set = csv_reader_dataset(train_filepaths, repeat=None)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([...])\n",
    "model.compile([...])\n",
    "model.fit(train_set, epochs=10, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_set)\n",
    "#새로운 샘플이 3개라고 가정\n",
    "#예측 위한 데이터는 레이블을 가지고 있지 않음(있더라도 케라스가 무시함)\n",
    "#데이터셋 대신에 넘파이 배열 사용도 가능\n",
    "new_set = test_set.take(3).map(lambda X,y: X) \n",
    "model.predict(new_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a short description of each method in the `Dataset` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● apply()              Applies a transformation function to this dataset.\n",
      "● as_numpy_iterator()  Returns an iterator which converts all elements of the dataset to numpy.\n",
      "● batch()              Combines consecutive elements of this dataset into batches.\n",
      "● cache()              Caches the elements in this dataset.\n",
      "● concatenate()        Creates a `Dataset` by concatenating the given dataset with this dataset.\n",
      "● element_spec()       The type specification of an element of this dataset.\n",
      "● enumerate()          Enumerates the elements of this dataset.\n",
      "● filter()             Filters this dataset according to `predicate`.\n",
      "● flat_map()           Maps `map_func` across this dataset and flattens the result.\n",
      "● from_generator()     Creates a `Dataset` whose elements are generated by `generator`.\n",
      "● from_tensor_slices() Creates a `Dataset` whose elements are slices of the given tensors.\n",
      "● from_tensors()       Creates a `Dataset` with a single element, comprising the given tensors.\n",
      "● interleave()         Maps `map_func` across this dataset, and interleaves the results.\n",
      "● list_files()         A dataset of all files matching one or more glob patterns.\n",
      "● map()                Maps `map_func` across the elements of this dataset.\n",
      "● options()            Returns the options for this dataset and its inputs.\n",
      "● padded_batch()       Combines consecutive elements of this dataset into padded batches.\n",
      "● prefetch()           Creates a `Dataset` that prefetches elements from this dataset.\n",
      "● range()              Creates a `Dataset` of a step-separated range of values.\n",
      "● reduce()             Reduces the input dataset to a single element.\n",
      "● repeat()             Repeats this dataset so each original value is seen `count` times.\n",
      "● shard()              Creates a `Dataset` that includes only 1/`num_shards` of this dataset.\n",
      "● shuffle()            Randomly shuffles the elements of this dataset.\n",
      "● skip()               Creates a `Dataset` that skips `count` elements from this dataset.\n",
      "● take()               Creates a `Dataset` with at most `count` elements from this dataset.\n",
      "● unbatch()            Splits elements of a dataset into multiple elements.\n",
      "● window()             Combines (nests of) input elements into a dataset of (nests of) windows.\n",
      "● with_options()       Returns a new `tf.data.Dataset` with the given options set.\n",
      "● zip()                Creates a `Dataset` by zipping together the given datasets.\n"
     ]
    }
   ],
   "source": [
    "for m in dir(tf.data.Dataset):\n",
    "    if not (m.startswith(\"_\") or m.endswith(\"_\")):\n",
    "        func = getattr(tf.data.Dataset, m)\n",
    "        if hasattr(func, \"__doc__\"):\n",
    "            print(\"● {:21s}{}\".format(m + \"()\", func.__doc__.split(\"\\n\")[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.2 TFRecord 포맷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대용량 데이터 저장 및 효율적 읽기 위해 텐서플로에서 선호하는 포맷\n",
    "- 크기가 다른 연속된 binary record를 저장하는 단순한 binary 포맷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `TFRecord` binary format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A TFRecord file is just a list of binary records. You can create one using a `tf.io.TFRecordWriter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.io.TFRecordWriter클래스로 TFRecord 만들기\n",
    "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"And this is the second record\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can read it using a `tf.data.TFRecordDataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# tf.data.TFRecordDataset을 사용해 하나 이상의 TFRecord읽기\n",
    "filepaths = [\"my_data.tfrecord\"] \n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read multiple TFRecord files with just one `TFRecordDataset`. By default it will read them one at a time, but if you set `num_parallel_reads=3`, it will read 3 at a time in parallel and interleave their records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'File 0 record 0', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 1 record 0', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 2 record 0', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 0 record 1', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 1 record 1', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 2 record 1', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 0 record 2', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 1 record 2', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 2 record 2', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 3 record 0', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 4 record 0', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 3 record 1', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 4 record 1', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 3 record 2', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 4 record 2', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "filepaths = [\"my_test_{}.tfrecord\".format(i) for i in range(5)]\n",
    "for i, filepath in enumerate(filepaths):\n",
    "    with tf.io.TFRecordWriter(filepath) as f:\n",
    "        for j in range(3):\n",
    "            f.write(\"File {} record {}\".format(i, j).encode(\"utf-8\"))\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(filepaths, num_parallel_reads=3)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.2.1 압축된 TFRecord 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFRecord 파일 압축\n",
    "# options 매개변수를 사용\n",
    "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"And this is the second record\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# 압축된 TFRecord 파일을 읽기 위해, 압축 형식 지정해야 함\n",
    "dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"],\n",
    "                                  compression_type=\"GZIP\")\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.2.2 프로토콜 버퍼 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 레코드는 어떤 이진 포맷도 사용 가능함\n",
    "- TFRecord는 직렬화된 프로토콜 버퍼(protocol buffer)를 담고 있음\n",
    "- 구글이 개발한 이식성과 확장성이 좋고 효율적인 이진 포맷\n",
    "- 구글 원격 프로시저 호출 시스템(gRPC)에 사용\n",
    "\n",
    "[참고] https://jeong-pro.tistory.com/190"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.2.3 텐서플로 프로토콜 버퍼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.3 입력 특성 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 신경망 입력\n",
    "    - 모든 특성을 수치 특성으로 변환 후 정규화\n",
    "    - 범주형, 텍스트 특성 변환 필요 \n",
    "    - 적용 방법\n",
    "        - 데이터 파일 준비하기 전에 처리하기\n",
    "        - 데이터 API로 데이터 적재할 때 동적으로 전처리 하기 (ex. map)\n",
    "        - 전처리 층을 모델에 포함시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 층을 모델에 포함시키기 (means, stds 전역 변수 이용 )\n",
    "means = np.mean(X_train, axis=0, keepdims=True)\n",
    "stds = np.std(X_train, axis=0, keepdims=True)\n",
    "eps = keras.backend.epsilon()\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Lambda(lambda inputs: (inputs-means)/(stds+eps)),\n",
    "    #[..] #다른층\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리를 위한 사용자 정의층\n",
    "class Standardization(keras.layers.Layer):\n",
    "    def adapt(self, data_sample):\n",
    "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
    "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization층을 모델에 추가 전, 각 특성에 대한 평균과 표준 편차 사용하기 위해\n",
    "# 데이터 샘플에 adapt() 메서드 호출\n",
    "# 데이터 샘플은 전체 데이터 셋을 대표할 만큼 충분히 커야하지만, 전체 훈련 세트일 필요 없음 \n",
    "# 일반적으로 랜덤하게 선택된 수 백개 샘플(주어진 문제마다 다름)\n",
    "std_layer = Standardization()\n",
    "std_layer.adapt(data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(std_layer)\n",
    "#[...] #모델을 구성\n",
    "model.comfile([...])\n",
    "model.fit([..])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 케라스 전처리층 API\n",
    "    - keras.layers.Normalization층 사용 가능\n",
    "    - 앞서 만든 Stadardization과 비슷하게 동작\n",
    "    - 먼저 층 만든 후, adopt()에서 샘플 데이터 전달, 데이터 셋에 적응 후 보통 층 처럼 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.3.1 원-핫 벡터를 사용해 범주형 특성 인코딩하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 범주형 특성 인코딩하기 \n",
    "    - 범주형 특성의 경우 신경망이 주입 전 인코딩 해야 함\n",
    "    - 범주 개수가 매우 작으므로 원-핫 인코딩 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어휘 사전(vocabulary) 정의: 가능한 모든 범주에 대한 리스트\n",
    "# ocean_proximity 특성값\n",
    "vocab = ['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1088, shape=(5,), dtype=int64, numpy=array([0, 1, 2, 3, 4])>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 범주에 해당하는 인덱스의 텐서 만들기\n",
    "indices = tf.range(len(vocab), dtype=tf.int64)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어휘사전과 각 인덱스를 전달하여 lookup 테이블을 위해 초기화 객체 만듦\n",
    "table_init =tf.lookup.KeyValueTensorInitializer(vocab, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oov(out-of-vocabulary) 버킷을 지정\n",
    "num_oov_buckets = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup 테이블 만들기\n",
    "# 초기화 객체와 oov 버킷을 지정\n",
    "# 어휘 사전에 없는 범주를 찾으면, lookup테이블이 계산한 이 범주의 해시값을 이용해 oov 버킷중 하나에 ㅏㄹ당\n",
    "# 인덱스는 알려진 범주 다음 부터 시작(5,6)\n",
    "table = tf.lookup.StaticVocabularyTable(table_init,num_oov_buckets)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* oov 버킷 사용 이유\n",
    "    - 범주 개수가 많고(예, 우편 번호, 사용자 등), dataset이 크거나, 범주가 자주 바뀌면 전체 범주 리스트를 구하기 어려울 수 있음\n",
    "    - 전체 훈련 데이터가 아닌 샘플 데이터를 기반으로 vocabulary를 정의하고, 샘플 데이터에 없는 다른 범주를 oov 버킷에 추가\n",
    "    - 훈련 중 모르는 범주가 많을 수록 더 많은 oov 버킷을 사용해야 함\n",
    "    - oov 버킷이 충분해야 충돌 발생 막을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1100, shape=(4,), dtype=int64, numpy=array([3, 5, 1, 1])>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lookup 테이블 이용해 입력 범주 특성을 원-핫 벡터로 인코딩\n",
    "# 입력 범주와 인덱스를 맵핑 \n",
    "categories = tf.constant(['NEAR BAY', 'DESERT', 'INLAND', 'INLAND'])\n",
    "# 'NEAR BAY'의 index: 3\n",
    "# 'DESERT'의 index: 5 -> 두개 oov 버킷 중 하나에 맵핑\n",
    "# 'INLAND'의 index: 1\n",
    "cat_indices = table.lookup(categories)\n",
    "cat_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1112, shape=(4, 7), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.one_hot()\n",
    "# 할당된 인덱스를 원-핫 인코딩\n",
    "# 인덱스 총 크기(vocabulary 크기 + oov 버킷 수 더함)를 지정해야 함\n",
    "cat_one_hot = tf.one_hot(cat_indices, depth=len(vocab) + num_oov_buckets)\n",
    "cat_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* keras.layers.TextVectorization 층\n",
    "    - adapt(): 샘플 데이터에서 vocabulary 추출 \n",
    "    - call(): 각 범주를 어휘 사전에 있는 인덱스로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.3.2 임베딩을 사용해 범주형 특성 인코딩하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 원-핫 벡터 한계\n",
    " - 원-핫 벡터의 크기 = 어휘 사전 길이 + oov 버킷 개수 \n",
    " - 어휘 사전이 클 경우 임베딩을 사용하여 인코딩하는 것이 효율적\n",
    "     - 범주 개수 <= 10: 일반적으로 원-핫 인코딩 사용\n",
    "     - 범주 개수 >= 50: 임베딩 선호\n",
    "     - 10 < 범주 개수 < 50: 두 방식 모두 실험하여, 최선의 결과 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 임베딩\n",
    "    - 범주를 표현하는 훈련 가능한 밀집 벡터\n",
    "    - 처음엔 랜덤하게 초기화 되어 있음 \n",
    "    - 임베팅 차원수는 하이퍼파라미터\n",
    "    - 범주가 유용하게 표현 되도록 임베딩이 훈련 -> 비슷한 범주는 경사 하강법이 더 가깝게 만듦\n",
    "    - 표현 학습(representation learning)\n",
    "    - 방법\n",
    "        - 각 범주의 임베딩을 담은 임베딩 행렬(embedding matrix)를 만들어 랜덤 초기화\n",
    "        - 범주와 oov 버킷 마다 하나의 행\n",
    "        - 임베딩 차원마다 하나의 열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 차원: 2 \n",
    "# 하이퍼파라미터\n",
    "# 실제 작업과 어휘 사전 크기에 따라 10~300차원을 가짐\n",
    "embedding_dim = 2\n",
    "embed_init = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(7, 2) dtype=float32, numpy=\n",
       "array([[0.6645621 , 0.44100678],\n",
       "       [0.3528825 , 0.46448255],\n",
       "       [0.03366041, 0.68467236],\n",
       "       [0.74011743, 0.8724445 ],\n",
       "       [0.22632635, 0.22319686],\n",
       "       [0.3103881 , 0.7223358 ],\n",
       "       [0.13318717, 0.5480639 ]], dtype=float32)>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 임베딩 행렬: 7x2 행렬\n",
    "# 변수에 저장하여 훈련 과정에서 경사하강법으로 학습\n",
    "embedding_matrix = tf.Variable(embed_init)\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1146, shape=(4,), dtype=int64, numpy=array([3, 5, 1, 1])>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = tf.constant(['NEAR BAY', 'DESERT', 'INLAND', 'INLAND'])\n",
    "cat_indices = table.lookup(categories)\n",
    "cat_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1147, shape=(4, 2), dtype=float32, numpy=\n",
       "array([[0.74011743, 0.8724445 ],\n",
       "       [0.3103881 , 0.7223358 ],\n",
       "       [0.3528825 , 0.46448255],\n",
       "       [0.3528825 , 0.46448255]], dtype=float32)>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.nn.embedding_lookup(): 임베딩 행렬에서 주어진 인덱스에 해당하는 행을 찾음\n",
    "tf.nn.embedding_lookup(embedding_matrix, cat_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* keras.layers.Embedding층 \n",
    "    - 케라스에서 학습 가능한 임베딩 행렬을 처리 하는 함수\n",
    "    - 임베딩 행렬을 랜덤 초기화하고, 어떤 범주 인덱스로 호출 될때, 임베딩 행렬에 있는 그 인덱스 행 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1161, shape=(4, 2), dtype=float32, numpy=\n",
       "array([[-0.0426784 , -0.04786297],\n",
       "       [ 0.01936141, -0.04267867],\n",
       "       [-0.04826153, -0.01568551],\n",
       "       [-0.04826153, -0.01568551]], dtype=float32)>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = keras.layers.Embedding(input_dim=len(vocab) + num_oov_buckets, \n",
    "                                   output_dim=embedding_dim)\n",
    "embedding(cat_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 특성을 처리하고, 각 범주(각 oov 버킷) 마다 임베딩을 학습하는 케라스 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_inputs = keras.layers.Input(shape=[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = keras.layers.Input(shape=[], dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_embed = keras.layers.Lambda(lambda cats: table.lookup(cats))(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoded_inputs = keras.layers.concatenate([regular_inputs,cat_embed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = keras.layers.Dense(1)(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Model(inputs=[regular_inputs, categories],\n",
    "                          outputs=[outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.3.3 케라스 전처리 층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 표준 케라스 전처리층 제공\n",
    "    - keras.layers.Normalization 층: 특성 표준화 수행(정의한 Standardization층 동일)\n",
    "    - TextVectorization 층: 입력에 있는 각 단어를 어휘 사전에 있는 인덱서를 인코딩\n",
    "    - keras.layers.Discretization 층: 연속적인 데이터를 몇 개 구간(bin)으로 나누고 각 구간을 원-핫 벡터로 인코딩\n",
    "        - 가격 데이터: 낮음, 중간, 높음 -> [1,0,0],[0,1,0],[0,0,1]로 인코딩\n",
    "        - 잃는 정보는 많지만, 어떤 경우는 연속적인 값으로 볼 때 확실하지 않은 패턴을 감지하는 데 도움\n",
    "    - 전처리 층 만들고 -> 샘플 데이터로 adapt() 호출 -> 일반적인 층처럼 모델에 적용\n",
    "    - 전처리 층 중 학습이 필요없는 층(예: Discretization)일 경우 \n",
    "        - 학습하는 동안 동결 상태로, 미분할 필요가 없음\n",
    "        - 모델의 시작 부분에만 사용되야 함\n",
    "        - Embedding 층을 훈련해야 한다면 사용자 전처리 층 안에 두지 말고, 별도에 모델에 추가\n",
    "- PreprocessingStage 클래스\n",
    "    - 여러 전처리 층을 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization = keras.layers.Normalization()\n",
    "discretization = keras.layers.Discretization([...])\n",
    "# 여러 전처리 층을 연결\n",
    "# 입력을 정규화한 후 이산화하는 전처리 파이프라인 \n",
    "pipeline = keras.layers.ProcessingStage([normalization, discretization])\n",
    "# 샘플 데이터에 적응\n",
    "pipeline.adapt(data_sample)\n",
    "# 모델 시작 부분에 일반적인 층 처럼 추가(미분 가능하지 않은 전처리 층을 포함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TextVectorization 층\n",
    "    - 단어 인덱스 대신 단어 카운트 벡터를 출력하는 옵션을 가짐\n",
    "        - 어휘 사전: ['and', 'basketball', 'more']\n",
    "        - 텍스트: 'more and more' -> 벡터: [1, 0, 2] //각 등장 횟수\n",
    "    - 이런 텍스트 표현은 단어의 순서를 완전히 무시 -> BOW(Bag of word)\n",
    "    - 'more and more basketball'\n",
    "        - 'and' 같이 중요하지 않은 단어의 빈도는 높음\n",
    "        - 'basketball' 같이 드물게 등장하는 단어는 중요한 단어\n",
    "        - 단어 카운트는 자주 등장하는 단어의 중요도를 낮춰 방향으로 정규화 필요\n",
    "        - TD-IDF\n",
    "            - 전체 샘플 수를 단어가 등장하는 훈련 샘플 개수로 나눈 로그를 계산 한 후 단어 카운트와 곱함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.4 TF 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.5 텐서플로 데이터셋(TFDS) 프로젝트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 널리 사용하는 dataset 손쉽게 사용 다운로드\n",
    "# 이미지, 텍스트, 번역, 오디오, 비디오 데이터 셋\n",
    "# https://homl.info/tfds 문서 참고(각 데이터 셋 설명, 전체 리스트)\n",
    "# tensorflow-datasets 라이브러리 별도 설치 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train'])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# tfds.load()\n",
    "# 데이터 로드하기\n",
    "dataset = tfds.load(name=\"mnist\")\n",
    "# 딕셔너리 형태로 반환(주로 훈련용, 테스트 용 데이터)\n",
    "dataset.keys() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = dataset[\"train\"], dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_train.shuffle(10000).batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image', 'label'])\n"
     ]
    }
   ],
   "source": [
    "for item in mnist_train:\n",
    "    print(item.keys())\n",
    "    #image = item['image']\n",
    "    #label = item['label']\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 케라스용 입력 데이터 만들기\n",
    "# 입력: 튜플 -> (특성, 레이블)\n",
    "mnist_train = mnist_train.shuffle(100000).batch(32)\n",
    "# map()함수 이용해 각 아이템의 특성과 레이블을 담은 딕셔너리를 튜플로 변환\n",
    "mnist_train = mnist_train.map(\n",
    "    lambda items: (items['image'], items['label']))\n",
    "mnist_trina = mnist_train.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load()함수에서 as_supervised=True 지정해도 가능\n",
    "dataset = tfds.load(name=\"mnist\", batch_size=32, as_supervised=True)\n",
    "mnist_train = dataset['train'].prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([...])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\n",
    "model.fit(mnist_train, epoch=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nav_menu": {
   "height": "264px",
   "width": "369px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
